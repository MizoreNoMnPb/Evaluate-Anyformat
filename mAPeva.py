# creator: Liu Hongkai
# usage： calculate the mAPl mAPm mAPs for Li Ting's AAAI26 paper, calculating the mAP for ultralytics YOLOv8 result using cocotools and mmratote
# dataset: DroneVehicle, M3FD

# hbb: horizontal bounding box(M3FD)
# mAP calculation: pycocotools
# prediction json file generate: ultralytics
# ultralytics will generate json files as this format, which matches the COCO result file format:
# [
#    {"image_id": 0, "category_id": 1, "bbox": [x, y, width, height], "score": 0.9},
#    {"image_id": 0, "category_id": 2, "bbox": [x, y, width, height], "score": 0.8},
#    {"image_id": 1, "category_id": 1, "bbox": [x, y, width, height], "score": 0.85},
#     ...
# ]
# especially，JSON files generated by ultralytics will take category_id to 1, 2,...
# However, the category_id in GT label text files is 0, 1, 2,..., so a +1 is needed.
# all of the json files' convertion is for GT, because customized datasets may not provide json files, but only text files.
# pycocotools's COCO class can load a json file in this format:
# [
#   "images": [ {"id": 0, "file_name": "00001.png"}, {"id": 1, "file_name": "00002.png"}, ...], (file_name is not necessary, but id is)
#   "annotations": [{
#           "id": 1, 
#           "image_id": 0, 
#           "category_id": 1,
#           "bbox": [x, y, width, height],
#           "area": width*height,
#           "score": 0.9,
#           "iscrowd": 0}, 
#           ...],
#   "categories": [{"id": 1, "name": "category1"}, {"id": 2, "name": "category2"}, ...], (name is not necessary, but id is)
#   "info": {"description": "COCO format dataset"},
#   "licenses": [{"id": 0, "name": "Unknown"}] (# not necessary, but id is)
# ]
# a ground truth (GT) json file's generation pipline is:
# 1. read the GT text files, which are in yolo format, and summarize the information in a list of dictionaries, (gen_GT_JSON function)
# 2. convert the list of dictionaries to a COCO format dictionary. (convert_list_json_to_coco_dict_json function)

# GT text files pattern (yolo format):
# image_id.txt:
# category_id x_center y_center width height(normalized)
# 0 0.5 0.5 0.1 0.1

# obb: oriented bounding box(DroneVehicle)
# mAP calculation: mmrotate
# prediction json file generate: ultralytics
# ultralytics will generate json files as this format:
# [
#    {"image_id": 0, "category_id": 1, "rbox": [x_center, y_center, width, height, anlge], "poly":[x1, y1, x2, y2, x3, y3, x4, y4], "score": 0.9},
#    {"image_id": 0, "category_id": 2, "rbox": [x_center, y_center, width, height, anlge], "poly":[x1, y1, x2, y2, x3, y3, x4, y4], "score": 0.9},
#    {"image_id": 1, "category_id": 1, "rbox": [x_center, y_center, width, height, anlge], "poly":[x1, y1, x2, y2, x3, y3, x4, y4], "score": 0.9},
#     ...
# ]
# especially，JSON files generated by ultralytics will take category_id to 1, 2,..., 
# However, the category_id in GT text files is 0, 1, 2,...
# as for the rbox, this project use a obb head which will convert to angle [-π/4, 3π/4](check /data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/ultralytics/nn/modules/head.py L768)
# and the mmratote use a [-π, 0] angle, so we need to convert the angle to [-π, 0] for mmratote evaluation.
# MOREOVER, mmrotate's eval is this format:
# det_results = [
#    [cls1_det, cls2_det, ...],  # for image 1
#    [cls1_det, cls2_det, ...],  # for image 2
#    ...
# ]  list[list[ndarry(m,6)]]
# cls_det = [
#     ndarray([x, y, width, height, angle, score],
#             [x, y, width, height, angle, score],
#             ...),  # rbox  
# ]  list[ndarray(m, 6)]
# gt_annotations = [
#    {"bboxes": ndarray([x, y, width, height, angle], dtype=float32),
#     "labels": ndarray([category_id], dtype=int64),
#     *"ignore_bboxes": ndarray([x, y, width, height, angle], dtype=float32), # optional, for ignore bboxes},
#     *"ignore_labels": ndarray([category_id], dtype=int64)},                 # optional, for ignore labels
#     },
#    {"bboxes": ndarray([x, y, width, height, angle], dtype=float32),
#     "labels": ndarray([category_id], dtype=int64)
#     },
#    ...    
# ]   list[dict]
# the index of the det_results and gt_annotations should match the image_id in the json file.
# So why using JSON files... Maybe for further usage?

import os
import time
import copy
os.environ["NO_ALBUMENTATIONS_UPDATE"] = "1"  # ignore warnings
import json
from typing import Union, Sequence, Optional

import cv2
import numpy as np
import torch
from torch import Tensor, BoolTensor
from PIL import Image
import pycocotools
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval
from pycocotools import mask as maskUtils
from mmcv.utils import ext_loader
import mmcv._ext


DeviceType = Union[str, torch.device]
ext_module = ext_loader.load_ext('_ext', ['box_iou_rotated']) # follow the mmcv's ext_loader.py, load the extension module

class RotatedCOCO(COCO):
    """
    A subclass of COCO
    that supports rotated bounding boxes.
    """
    def __init__(self, annotation_file = None):
        super().__init__(annotation_file)
        
    def loadRes(self, resFile):
        '''
        ultralytics's result format:
            res = [ {'image_id': 1, 
                    'category_id': 1, 
                    'score': 0.91736, 
                    'rbox': [304.51, 417.324, 29.041, 66.037, -3.090592653589793],  
                    ------- rboxs' angle is [-π/4, 3π/4](le135 generated by ultralytics), which is not really follow the COCO format, -------
                    ------- please use ..._convert_obb_angles.json, it will be [-π / 2, -π / 2](le90), and dont have the poly part. -------
                    'poly': [317.335, 451.037, 320.69, 385.086, 291.686, 383.61, 288.331, 449.562]},
                    ...]: list of dicts
        '''
        res = RotatedCOCO()
        res.dataset['images'] = [img for img in self.dataset['images']]
        
        print('Loading and preparing results...')
        tic = time.time()
        if type(resFile) == str:
            with open(resFile, 'r') as f:
                anns = json.load(f)
        elif type(resFile) == np.array:
            anns = self.loadNumpyAnnotations(resFile)
        else:
            anns = resFile
        assert type(anns) == list, 'results in not an array of objects'
        annsImgIds = [ann['image_id'] for ann in anns]
        annsImgIds = set(annsImgIds)
        resImgIds = set(self.getImgIds())
        uncontained = annsImgIds - resImgIds
        print('Annotations contain images not in results: {}'.format(uncontained))
        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \
               'Results do not correspond to current coco set'

        if 'bbox' not in anns[0]:
            raise Exception('Results do not have bbox field, this class only supports bbox results')
        else:
            res.dataset['categories'] = copy.deepcopy(self.dataset['categories'])
            for id, ann in enumerate(anns):
                bb = ann['bbox']
                x, y, w, h, angle = bb[0], bb[1], bb[2], bb[3], bb[4] 
                ann['area'] = w * h
                ann['id'] = id + 1 
                ann['iscrowd'] = 0
        print('DONE (t={:0.2f}s)'.format(time.time() - tic))
        
        res.dataset['annotations'] = anns
        res.createIndex()
        return res        
        
            
    def loadNumpyAnnotations(self, data):
        print('Converting ndarray to lists...')
        assert(type(data) == np.ndarray)
        print(data.shape)
        assert(data.shape[1] == 8)
        N = data.shape[0]
        ann = []
        for i in range(N):
            if i % 1000000 == 0:
                print('{}/{}'.format(i,N))
            ann += [{
                'image_id'  : int(data[i, 0]),
                'bbox'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4], data[i, 5] ],
                'score' : data[i, 6],
                'category_id': int(data[i, 7]),
                }]
        return ann
        

class RotatedCocoEval(COCOeval):
    """
    A subclass of COCOeval
    that supports rotated bounding boxes.
    """
    def computeIoU(self, imgId, catId):
        p = self.params
        if p.useCats:
            gt = self._gts[imgId, catId]
            dt = self._dts[imgId, catId]
        else:
            gt = [_ for cId in p.catIds for _ in self._gts[imgId, cId]]
            dt = [_ for cId in p.catIds for _ in self._dts[imgId, cId]]
        if len(gt) == 0 and len(dt) == 0:
            return []
        inds = np.argsort([-d['score'] for d in dt], kind='mergesort')
        dt = [dt[i] for i in inds]
        if len(dt) > p.maxDets[-1]:
            dt = dt[0:p.maxDets[-1]]

        if p.iouType == 'segm':
            # For segm, its same with original COCOeval
            g = [g['segmentation'] for g in gt]
            d = [d['segmentation'] for d in dt]
            # compute iou between each dt and gt region
            iscrowd = [int(o['iscrowd']) for o in gt]
            ious = maskUtils.iou(d, g, iscrowd)
        elif p.iouType == 'bbox':
            # Modified for Rotated Box
            g = [g['bbox'] for g in gt]
            d = [d['bbox'] for d in dt]
            # Convert List[List[float]] to Tensor for iou compute
            g = torch.tensor(g, dtype=torch.float32)
            d = torch.tensor(d, dtype=torch.float32)
            ious = self.box_iou_rotated(d, g)
            
        else:
            raise Exception('unknown iouType for iou computation')

        return ious
        
        
    def box_iou_rotated(self, 
                        det_boxes: torch.Tensor, 
                        gt_boxes:  torch.Tensor,
                        mode: str = 'iou',
                        aligned: bool = False,
                        clockwise: bool = True) -> torch.Tensor:
        
        assert mode in ['iou', 'iof']
        mode_dict = {'iou': 0, 'iof': 1}
        mode_flag = mode_dict[mode]
        rows = det_boxes.size(0)
        cols = gt_boxes.size(0)
        if aligned:
            ious = det_boxes.new_zeros(rows)
        else:
            if det_boxes.device.type == 'mlu':
                ious = det_boxes.new_zeros([rows, cols])
            else:
                ious = det_boxes.new_zeros(rows * cols)
        if not clockwise:
            flip_mat = det_boxes.new_ones(det_boxes.shape[-1])
            flip_mat[-1] = -1
            det_boxes = det_boxes * flip_mat
            gt_boxes = gt_boxes * flip_mat
        if det_boxes.device.type == 'npu':
            scale_mat = det_boxes.new_ones(det_boxes.shape[-1])
            scale_mat[-1] = 1.0 / 0.01745329252
            det_boxes = det_boxes * scale_mat
            gt_boxes = gt_boxes * scale_mat
        det_boxes = det_boxes.contiguous()
        gt_boxes = gt_boxes.contiguous()
        ext_module.box_iou_rotated(
            det_boxes, gt_boxes, ious, mode_flag=mode_flag, aligned=aligned)
        if not aligned:
            ious = ious.view(rows, cols)
        return ious
    
    def evaluate(self):
        return super().evaluate()
    def accumulate(self, p = None):
        return super().accumulate(p)
    def summarize(self):
        return super().summarize()
        

def gen_hbb_GT_coco_JSON(dataset):
    
    label_path = '/data/LiTing/YOLO-RGBT/dataset_yolo/hbb/M3FD_yolo/val/visible/labels'
    image_path = '/data/LiTing/YOLO-RGBT/dataset_yolo/hbb/M3FD_yolo/val/visible/images'
    output_json = '/data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/runs/val/M3FD_json/M3FD_GT.json'
    
    annotations = []
    
    for filename in os.listdir(label_path):
        if filename.endswith('.txt'):
            image_id = int(filename.split('.')[0])
            image_file = f'{image_id:05d}.png'
            image_file_path = os.path.join(image_path, image_file)
            
            try:
                with Image.open(image_file_path) as img:
                    width, height = img.size
            except FileNotFoundError:
                print(f"Image file {image_file_path} not found. Skipping...")
                continue
            
            with open(os.path.join(label_path, filename), 'r') as f:
                lines = f.readlines()
            
            for line in lines:
                parts = line.strip().split()
                                
                category_id = int(parts[0]) + 1
                x_center = float(parts[1]) * width
                y_center = float(parts[2]) * height
                bbox_width = float(parts[3]) * width
                bbox_height = float(parts[4]) * height
                bbox = [
                    x_center - bbox_width / 2, 
                    y_center - bbox_height / 2, 
                    bbox_width, 
                    bbox_height
                ]
                score = 1.0
                annotations.append({
                    "image_id": image_id,
                    "category_id": category_id,
                    "bbox": bbox,
                    "score": score
                })
            
    with open(output_json, 'w') as jf:
        json.dump(annotations, jf)
        

# DONT use this function to convert the results json files generated by ultralytics, its for GT files.
def convert_list_json_to_coco_dict_json(jsonfile_path):
    with open(jsonfile_path, 'r') as f:
        data = json.load(f)
    
    image_ids = set()
    category_ids = set()
    
    for pred in data:
        image_ids.add(pred['image_id'])
        category_ids.add(pred['category_id'])
    
    coco_format =  {
        "images": [{"id": img_id} for img_id in image_ids],
        "annotations": [],
        "categories": [{"id": cat_id} for cat_id in category_ids],
        "info": {"description": "Converted from list format to COCO dict format"},
        "licenses": [{"id": 0, "name": "Unknown"}]
    }
    
    for idx, pred in enumerate(data):
        bbox = pred['bbox']
        annotation = {
            "id": idx + 1,
            "image_id": pred['image_id'],
            "category_id": pred['category_id'],
            "bbox": bbox,
            "area": bbox[2] * bbox[3],  # width * height
            "score": pred['score'],
            "iscrowd": 0  # Assuming no crowd annotations, necessary for COCO format
        }
        coco_format["annotations"].append(annotation)
    
    output_json_path = jsonfile_path.replace('.json', '_coco.json')
    with open(output_json_path, 'w') as f:
        json.dump(coco_format, f, indent=4)

    print(f"Converted {jsonfile_path} to COCO dict format and saved as {output_json_path}")
    

def M3FD_mAP_calculation(pre_json_path):
    print("M3FD_model mAP calculation:")
    coco_gt_m3fd = COCO('/data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/runs/val/M3FD_json/M3FD_GT_coco.json')
    
    # actually, ultralytics's json is right, it's the minimum requirement of COCO format's result load, if using converted _coco.json, it will cause error.
    coco_pre_m3fd = coco_gt_m3fd.loadRes(pre_json_path)
      
    coco_eval_m3fd = COCOeval(coco_gt_m3fd, coco_pre_m3fd, 'bbox') 
    
    coco_eval_m3fd.evaluate()
    coco_eval_m3fd.accumulate()
    coco_eval_m3fd.summarize()
    
    print("")
    print("M3FD_baseline mAP calculation:")
    coco_pre_m3fd_baseline = coco_gt_m3fd.loadRes('/data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/runs/val/M3FD_json/M3FD-baseline/predictions.json')
    coco_eval_m3fd_baseline = COCOeval(coco_gt_m3fd, coco_pre_m3fd_baseline, 'bbox')
    
    coco_eval_m3fd_baseline.evaluate()
    coco_eval_m3fd_baseline.accumulate()
    coco_eval_m3fd_baseline.summarize()
        

def gen_obb_GT(dataset):
    '''
    Used for mmrotate.evaluation, the GT is in a list of dictionaries format,
    this function won't generate a json file, but a list of dictionaries.
    '''
    
    label_path = '/data/LiTing/YOLO-RGBT/dataset_yolo/obb/DroneVehicle_840/val/visible/labels'
    image_path = '/data/LiTing/YOLO-RGBT/dataset_yolo/obb/DroneVehicle_840/val/visible/images'
    output_json = '/data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/runs/val/DV_json/DV_GT.json'
    
    annotations = []
    class_num = 5
    
    for filename in os.listdir(label_path):
        imgname = filename.split('.')[0] + '.jpg'
        with Image.open(os.path.join(image_path, imgname)) as img:
            width, height = img.size
        with open(os.path.join(label_path, filename), 'r') as f:
            lines = f.readlines()
        
        annotation = {
            "bboxes" : np.zeros((0, 5), dtype=np.float32),  # [x_center, y_center, width, height, angle]
            "labels" : np.zeros((0,), dtype=np.int64),     
        }
        for line in lines:
            parts = line.strip().split()
            
            category_id = int(parts[0])
            norm_points = [float(p) for p in parts[1:]]
            abs_points = np.array(norm_points).reshape(-1, 2) * np.array([width, height])
            
            rect = cv2.minAreaRect(abs_points.astype(np.float32))
            (x, y), (w, h), angle = rect
            if angle > np.pi / 2:
                angle -= np.pi
            
            annotation["bboxes"] = np.append(annotation["bboxes"], np.array([[x, y, w, h, angle]], dtype=np.float32), axis=0)
            annotation["labels"] = np.append(annotation["labels"], np.array([category_id], dtype=np.int64))
        
        annotations.append(annotation)
    
    # sort the box 

    return annotations          


def get_obb_GT_coco_json(dataset):            
    
    label_path = '/data/LiTing/YOLO-RGBT/dataset_yolo/obb/DroneVehicle_840/val/visible/labels'
    image_path = '/data/LiTing/YOLO-RGBT/dataset_yolo/obb/DroneVehicle_840/val/visible/images'
    output_json = '/data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/runs/val/DV/DV_GT.json'
    
    annotations = []
    
    for filename in os.listdir(label_path):
        imgname = filename.split('.')[0] + '.jpg'
        with Image.open(os.path.join(image_path, imgname)) as img:
            width, height = img.size
        with open(os.path.join(label_path, filename), 'r') as f:
            lines = f.readlines()
        
        for line in lines:
            parts = line.strip().split()
            
            category_id = int(parts[0]) + 1  # COCO format requires category_id to start from 1
            norm_points = [float(p) for p in parts[1:]]
            abs_points = np.array(norm_points).reshape(-1, 2) * np.array([width, height])
            
            rect = cv2.minAreaRect(abs_points.astype(np.float32))
            (x, y), (w, h), angle = rect
            if angle > np.pi / 2:
                angle -= np.pi
            
            bbox = [x - w / 2, y - h / 2, w, h]
            score = 1.0
            
            annotations.append({
                "image_id": int(filename.split('.')[0]),
                "category_id": category_id,
                "rbox": [x, y, w, h, angle],
                "poly": abs_points.flatten().tolist(),
                "score": score
            })
    
    image_ids = set()
    category_ids = set()
    
    for truth in annotations:
        image_ids.add(truth['image_id'])
        category_ids.add(truth['category_id'])
        
    coco_format =  {
        "images": [{"id": img_id} for img_id in image_ids],
        "annotations": [],
        "categories": [{"id": cat_id} for cat_id in category_ids],
        "info": {"description": "Converted from list format to COCO dict format"},
        "licenses": [{"id": 0, "name": "Unknown"}]
    }
    
    for idx, truth in enumerate(annotations):
        rbox = truth['rbox']
        annotation = {
            "id": idx + 1,
            "image_id": truth['image_id'],
            "category_id": truth['category_id'],
            "bbox": rbox,
            ""
            "score": truth['score'],
            "iscrowd": 0  # Assuming no crowd annotations, necessary for COCO format
        }
        coco_format["annotations"].append(annotation)
                                        
    with open(output_json, 'w') as f:
        json.dump(coco_format, f, indent=4)
    print(f"Saved {label_path} to COCO dict format and saved as {output_json}")

# this is for converting the obb angle from ultralytics's [-π/4, 3π/4] to mmrotate's [-π, 0], ONLY for predictions.json files generated by ultralytics.
# as for the question of why convert 'rbox' to 'bbox',
# that's because mmrotate's rotateCOCOval class is a subclass of COCOeval, it will use the COCOeval's evaluate function, so do the format.
# DONT use this function to convert the GT files.
def convert_obb_angles_and_format(jsonfile_path):
    
    '''
    As the comments at the top of this file, the ultralytics's obb angle is in [-π/4, 3π/4],
    but the mmrotate's obb angle is in [-π, 0], so we need to convert the angle.
    '''
    with open(jsonfile_path, 'r') as f:
        data = json.load(f)
    
    for pred in data:
        rbox = pred['rbox']
        angle = rbox[4]
        
        # Convert angle from [-π/4, 3π/4] to [-π, 0]
        if angle > np.pi / 2:
            angle -= np.pi
        
        rbox[4] = angle
        pred['rbox'] = rbox
    
    new_data = []
    
    for pred in data:
        new_pred = {
            "image_id": pred['image_id'],
            "category_id": pred['category_id'],
            "bbox": pred['rbox'],
            "score": pred['score']
        }
        new_data.append(new_pred)
    
    output_json_path = jsonfile_path.replace('.json', '_coco.json')
    with open(output_json_path, 'w') as f:
        json.dump(new_data, f)

    
def DV_mAP_calculation(pre_json_path, baseline_json_path):

    print("DV_model mAP calculation:")
    coco_gt_dv = RotatedCOCO('/data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/runs/val/DV/DV_GT_coco.json')
    
    # actually, ultralytics's json is right, it's the minimum requirement of COCO format's result load, if using converted _coco.json, it will cause error.
    coco_pre_dv = coco_gt_dv.loadRes(pre_json_path)
      
    coco_eval_dv = RotatedCocoEval(coco_gt_dv, coco_pre_dv, 'bbox') 
    
    coco_eval_dv.params.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 16 ** 2], [16 ** 2, 32 ** 2], [32 ** 2, 1e5 ** 2]]
    coco_eval_dv.params.iouThrs = np.linspace(.25, 0.95, int(np.round((0.95 - .25) / .05)) + 1, endpoint=True)
    
    coco_eval_dv.evaluate()
    coco_eval_dv.accumulate()
    coco_eval_dv.summarize()
    
    print("")
    print("DV_baseline mAP calculation:")
    coco_pre_dv_baseline = coco_gt_dv.loadRes(baseline_json_path)
    coco_eval_dv_baseline = RotatedCocoEval(coco_gt_dv, coco_pre_dv_baseline, 'bbox')
    
    coco_eval_dv_baseline.evaluate()
    coco_eval_dv_baseline.accumulate()
    coco_eval_dv_baseline.summarize()
    


def try_coco_rotate_val(pre_json_path):
    
    gt_json_path = '/data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/runs/val/DV_json/DV_GT_coco.json'
    
    gt_container = RotatedCOCO(gt_json_path)
    pre_container = gt_container.loadRes(pre_json_path)
    
    val_container = RotatedCocoEval(gt_json_path, pre_json_path, 'bbox') 
    
    val_container.evaluate()
    val_container.accumulate()
    val_container.summarize()
        
def convertion(path):
    
    with open(path, 'r') as f:
        data = json.load(f)
        
    
    new_data =  {
        "images": data['images'],
        "annotations": [],
        "categories": data['categories'],
        "info": {"description": "Converted from list format to COCO dict format"},
        "licenses": [{"id": 0, "name": "Unknown"}]
    }
    
    annotations = data['annotations']
    
    for idx, pred in enumerate(annotations):
        annotation = {
            "id": idx + 1,
            "image_id": pred['image_id'],
            "category_id": pred['category_id'],
            "bbox": pred['rbox'],
            "area": pred['rbox'][2] * pred['rbox'][3],
            "score": pred['score'],
            "iscrowd": 0  # Assuming no crowd annotations, necessary for COCO format
        }
        
        new_data["annotations"].append(annotation)
    
    output_json_path = path.replace('.json', '_coco.json')
    with open(output_json_path, 'w') as f:
        json.dump(new_data, f, indent=4)
    print(f"Converted {path} to COCO dict format and saved as {output_json_path}")
        

def main():
    # Uncomment the following lines to generate GT JSON files if they do not exist, check the directory first.   
    # gen_hbb_GT_coco_json('M3FD')
    # convert_list_json_to_coco_dict_json('/data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/runs/val/M3FD_json/M3FD_GT.json')
    # get_obb_GT_coco_json('DroneVehicle')
    # convert_obb_angles_and_format('/data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/runs/val/DV/DV-yolov8l_baseline2/predictions.json')
    # convert_obb_angles_and_format('/data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/runs/val/DV/DV-yolov8l_pretrained-fhg2-test/predictions.json')
    
    M3FD_mAP_calculation(pre_json_path='/data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/runs/val/M3FD_json/M3FD-yolov8l_pretrained-fhg2-json/predictions.json')
    
    pre_path = '/data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/runs/val/DV/DV-yolov8l_pretrained-fhg2-test/predictions_coco.json'
    baseline_path = '/data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/runs/val/DV/DV-yolov8l_baseline2/predictions_coco.json'
    DV_mAP_calculation(pre_path, baseline_path)

    
    # with open('/data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/runs/val/DV/DV-yolov8l_baseline2/predictions_coco.json', 'r') as f:
    #     gt_json = json.load(f)
    # # 
    # new_json = []
    # for pre in gt_json:
    #     if pre['image_id'] in [1320, 1322]:
    #         continue
    #     new_json.append(pre)
    # # 
    # with open('/data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/runs/val/DV/DV-yolov8l_baseline2/predictions_coco.json', 'w') as f:
    #     json.dump(new_json, f)
    #     print("Removed annotations and images with image_id 1320 and 1322 from GT JSON file.")
    
    # with open('/data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/runs/val/DV/DV-yolov8l_pretrained-fhg2-test/predictions_coco.json', 'r') as f:
    #     gt_json = json.load(f)
    # # 
    # new_json = []
    # for pre in gt_json:
    #     if pre['image_id'] in [1320, 1322]:
    #         continue
    #     new_json.append(pre)
    # # 
    # with open('/data/LiTing/YOLO-RGBT/YOLOv11-RGBT-master/runs/val/DV/DV-yolov8l_pretrained-fhg2-test/predictions_coco.json', 'w') as f:
    #     json.dump(new_json, f)
    #     print("Removed annotations and images with image_id 1320 and 1322 from GT JSON file.")

if __name__ == '__main__':
    main()
